# -*- coding: utf-8 -*-
"""Testing Whisper ASR WER

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jdkMf6BSlc83_LX4rBBeMIf2exOlst_U
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install datasets
!pip install evaluate
!pip install jiwer

!python -c "from huggingface_hub.hf_api import HfFolder; HfFolder.save_token('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')"
!pip install huggingface_hub

from transformers import WhisperProcessor, WhisperForConditionalGeneration
from datasets import load_dataset, DatasetDict
import pandas as pd
from ast import literal_eval
from datasets import Audio
from tqdm.notebook import tqdm
from functools import partial
import evaluate
from evaluate import load
from transformers.models.whisper.english_normalizer import BasicTextNormalizer

# load model and processor
processor = WhisperProcessor.from_pretrained(
    "openai/whisper-small", language="english", task="transcribe"
)
model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-small")
model.config.forced_decoder_ids = None

dataset = load_dataset(
    "hthomas7/audio_dataset_8", split="test"
)

dataset['phrase']

dataset = dataset.cast_column("audio", Audio(sampling_rate=16000))

model.generate = partial(
    model.generate, language="english", task="transcribe", use_cache=True
)

arr = [56, 102, 114, 133, 148, 149, 153, 155, 161, 190, 198]

transcription_list = []
for i in tqdm(range(200)):
  sample = dataset['audio'][i]
  #print("sample:", sample)
  input_features = processor(sample["array"], sampling_rate=sample["sampling_rate"], return_tensors="pt").input_features
  #print("input features:", input_features)
  predicted_ids = model.generate(input_features) # generate token ids
  #print("predicted ids:", predicted_ids)
  transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)
  if len(transcription) == 1:
    transcription = transcription[0]
  print(transcription)
  transcription_list.append(transcription)

# transcription_list = []
# for i in tqdm(arr):
#   sample = dataset['audio'][i]
#   #print("sample:", sample)
#   input_features = processor(sample["array"], sampling_rate=sample["sampling_rate"], return_tensors="pt").input_features
#   #print("input features:", input_features)
#   predicted_ids = model.generate(input_features) # generate token ids
#   #print("predicted ids:", predicted_ids)
#   transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)
#   print(transcription)
#   transcription_list.append(transcription)

df = pd.DataFrame({"PredLabels": transcription_list, "RefLabels": dataset['phrase']})
df.head()
df.to_csv("/content/drive/MyDrive/audio_pred_and_ref.csv", index=False)

CSV_FILE_PATH = "/content/drive/MyDrive/AI ML Bootcamp - Haripriya/Copy of audio_pred_and_ref.csv"
df = pd.read_csv(CSV_FILE_PATH, index_col=[0])
df.head()

df = df.T

df[0][198]

df[0].apply(lambda x: literal_eval(x))

(df[0].apply(lambda x: len(literal_eval(x))) == 1).sum()

print("How many files had multiple output strings:", (df[0].apply(lambda x: ", " in x).sum()))

df[0] = df[0].apply(lambda x: literal_eval(x)[0])
pred_labels = df[0].values
reference_labels = df[1].values

pred_labels = df["PredLabels"].values
reference_labels = df["RefLabels"].values

wer = load("wer")

normalizer = BasicTextNormalizer()

pred_str_norm = [normalizer(pred) for pred in pred_labels]
label_str_norm = [normalizer(label) for label in reference_labels]

pred_labels

norm_wer = wer.compute(predictions=pred_str_norm, references=label_str_norm)

ortho_wer = wer.compute(predictions=pred_labels, references=reference_labels)

print(100*ortho_wer)
print(100*norm_wer)